<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Per aspera ad astra."><title>统计学习方法笔记5 SVM | Blog - Lucius</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-174277081-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">统计学习方法笔记5 SVM</h1><a id="logo" href="/.">Blog - Lucius</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">统计学习方法笔记5 SVM</h1><div class="post-meta">2020-04-12<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><a class="disqus-comment-count" data-disqus-identifier="2020/04/12/统计学习方法笔记5-SVM/" href="/2020/04/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B05-SVM/#disqus_thread"></a><div class="post-content"><h1 id="线性可分支持向量机-amp-硬间隔最大化"><a href="#线性可分支持向量机-amp-硬间隔最大化" class="headerlink" title="线性可分支持向量机&amp;硬间隔最大化"></a>线性可分支持向量机&amp;硬间隔最大化</h1><h2 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><p>给定线性可分训练数据集，通过间隔最大化或者等价地求解相应的凸二次规划问题得到的分离超平面为$w^{*} \cdot x + b^{*} = 0$和分类决策函数$f(x) = \textrm{sign}(w^{*} \cdot x + b^{*})$称为线性可分支持向量机。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="函数间隔-amp-几何间隔"><a href="#函数间隔-amp-几何间隔" class="headerlink" title="函数间隔&amp;几何间隔"></a>函数间隔&amp;几何间隔</h2><p>超平面关于样本点的函数间隔$\hat{\gamma_i} = y_i(w\cdot x_i + b)$，关于训练集额函数间隔为$\hat{\gamma} = \min_{i = 1, \dots, N}\hat{\gamma_{i}}$。</p>
<p>几何间隔将其规范化，超平面关于样本点的几何间隔${\gamma_i} = y_i(\frac{w}{|w|}\cdot x_i + \frac{b}{|w|})$，关于训练集额函数间隔为${\gamma} = \min_{i = 1, \dots, N}{\gamma_{i}}$。</p>
<h2 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h2><p>SVM基本思想时求解能正确划分数据集并且几何间隔最大的分离超平面。</p>
<p>线性可分SVM可以转化成如下最优化问题<br>$$<br>\min_{w, b} \frac{1}{2}|w|^2 \ \textrm{s.t.}\ y_i(w \cdot x_i + b) - 1 \ge 0<br>$$</p>
<a id="more"></a>

<h2 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h2><p>首先构建拉格朗日函数<br>$$<br>L(w, b, \alpha) = \frac{1}{2}|w|^2 - \sum_{i = 1}^N \alpha_i y_i (w \cdot x_i + b ) + \sum_{i=1}^N \alpha_i<br>$$<br>其中$\alpha_i \ge 0, \ i = 1, \dots, N$。</p>
<p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题<br>$$<br>\max_{\alpha}\min_{w, b} L(w,b,\alpha)<br>$$<br>先求$\min_{w, b}L(w, b, \alpha)$<br>$$<br>\nabla_{w} L(w, b, \alpha) = w - \sum_{i = 1}^{N} \alpha_i y_i x_i = 0<br>$$</p>
<p>$$<br>\nabla_b L(w, b, \alpha) = -\sum_{i = 1}^{N} \alpha_i y_i = 0<br>$$</p>
<p>得到$w = \sum_{i = 1}^{N} \alpha_i y_i x_i$, $\alpha_i y_i = 0$ 。</p>
<p>代入得到<br>$$<br>\begin{aligned}<br>L(w, b, \alpha) &amp; = \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \alpha_i\alpha_j y_i y_j (x_i\cdot x_j) -\sum_{i = 1}^N \alpha_i y_i ((\sum_{j=1}^N \alpha_j y_j x_j) x_i + b) + \sum_{i = 1}^N \alpha_i \<br>&amp; = - \frac{1}{2} \sum_{i = 1}^N \sum_{j = 1}^N \alpha_i\alpha_j y_i y_j (x_i\cdot x_j)  + \sum_{i = 1}^N \alpha_i<br>\end{aligned}<br>$$<br>求$\min_{w,b}L(w, b, \alpha)$对$\alpha$的极大，即时对偶问题。将目标函数由极大转换成求最小，求得到<br>$$\min_{\alpha}  \frac{1}{2} \sum_{i=1}^N \sum_{j = 1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) - \sum_{i = 1}^N \alpha_i$$<br>$$\textrm{s.t.} \sum_{i=1}^N \alpha_i y_i = 0 $$<br>$$\alpha_i \ge 0, \ i = 1, \dots, N$$</p>
<p>设$\alpha^{*}$是对偶问题的解，KKT条件成立，得到<br>$$<br>\begin{aligned}<br>\nabla_{w}L(w^{*}, b^{*}, \alpha^{*}) = w^{*} - \sum_{i=1}^N \alpha_i^{*}y_i x_i = 0 \<br>\nabla_b L(w^{*}, b^{*}, \alpha^{*}) = \sum_{i=1}^N \alpha_i^{*} y_i = 0 \<br> \alpha_i^{*}(y_i(w^{*} \cdot x_i + b^{*})-1) = 0, \ i = 1, \dots, N \<br>y_i(w^{*} \cdot x_i + b^{*})-1 \ge 0, \ i = 1, \dots, N \<br>\alpha_i \ge 0,\  i = 1, \dots, N<br>\end{aligned}<br>$$<br>由此得到$w^{*} =  \sum_{i=1}^N \alpha_i^{*}y_i x_i$。其中至少有一个$\alpha_j^{*} &gt; 0$，对它有$y_j(w^{*}\cdot x_j + b^{*}) -1 = 0$。注意到$y_j^2 = 1$，所以有$b^{*} = y_j - \sum_{i=1}^N \alpha_iy_i(x_i \cdot x_j)$。</p>
<p>分离超平面可以携程<br>$$<br>\sum_{i=1}^N \alpha_i^{*}y_i(x\cdot x_i) + b^{*} = 0<br>$$<br>分类决策函数可以写成<br>$$<br>f(x) = \textrm{sign}(\sum_{i=1}^N \alpha_iy_i(x\cdot x_i) + b^{*})<br>$$<br>它只依赖于输入$x$和训练样本输入的内积。</p>
<p>数据集中$\alpha_i&gt;0$的样本点称为支持向量。</p>
<h1 id="线性支持向量机-amp-软间隔最大化"><a href="#线性支持向量机-amp-软间隔最大化" class="headerlink" title="线性支持向量机&amp;软间隔最大化"></a>线性支持向量机&amp;软间隔最大化</h1><p>假设训练数据集不是线性可分的，引入松弛变量$\xi$<br>$$<br>\begin{aligned}<br>\min_{w, b, \xi} &amp; \frac{1}{2}|w|^2 + C \sum_{i=1}^N \xi_i \<br>\textrm{s.t.} &amp; y_i(w{*}x_i + b) \ge 1 - \xi_i,\ i = 1, \dots, N \<br>&amp; \xi_i \ge 0 ,\ i = 1, \dots, N<br>\end{aligned}<br>$$<br>拉格朗日函数<br>$$<br>L(w, b, \xi, \alpha, \mu) = \frac{1}{2}|w|^2 + C\sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i( y_i(w {*} x_i + b) - 1 + \xi_i) - \sum_{i=1}^N \mu_i \xi_i<br>$$<br>其中$\alpha_i \ge 0, \mu_i \ge 0$。</p>
<p>先对$w, b, \xi$求极小<br>$$<br>\begin{aligned}<br> \nabla_w L(w, b, \xi, \alpha, \mu) = w - \sum_{i=1}^N \alpha_i y_i x_i = 0\<br> \nabla_b L(w, b, \xi, \alpha, \mu) = -\sum_{i=1}^N \alpha_i y_i = 0 \<br> \nabla_\xi L(w, b, \xi, \alpha, \mu) = C - \alpha_i - \mu_i = 0<br>\end{aligned}<br>$$<br>得到<br>$$<br>\begin{aligned}<br> w = \sum_{i=1}^N \alpha_i y_i x_i \<br> \sum_{i=1}^N \alpha_i y_i = 0 \<br> C - \alpha_i - \mu_i = 0<br>\end{aligned}<br>$$<br>代入得到<br>$$<br>\min_{w, b, \xi} L(w, b, \xi, \alpha, \mu) = -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i<br>$$<br>得到对偶最优化为<br>$$<br>\begin{aligned}<br>\max_{\alpha}&amp;  -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i \<br>\textrm{s.t.} &amp; \sum_{i=1}^N \alpha_i y_i = 0 \<br>&amp; C - \alpha_i - \mu_i = 0, \ i = 1, \dots, N \<br>&amp; \alpha_i \ge 0 , \ i = 1, \dots, N \<br>&amp; \mu_i \ge 0 , \ i = 1, \dots, N \<br>\end{aligned}<br>$$<br>消去$\mu$<br>$$<br>\begin{aligned}<br>\max_{\alpha}&amp;  -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) + \sum_{i=1}^N \alpha_i \<br>\textrm{s.t.} &amp; \sum_{i=1}^N \alpha_i y_i = 0 \<br>&amp; 0 \le \alpha_i \le C , \ i = 1, \dots, N \<br>\end{aligned}<br>$$<br>设$\alpha$是对偶问题的一个解，满足KKT条件<br>$$<br>\begin{aligned}<br> \nabla_w L(w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*}) = w^{*} - \sum_{i=1}^N \alpha_i^{*} y_i x_i = 0\<br> \nabla_b L(w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*})  = -\sum_{i=1}^N \alpha_i^{*} y_i = 0 \<br> \nabla_\xi L(w^{*}, b^{*}, \xi^{*}, \alpha^{*}, \mu^{*})  = C - \alpha_i^{*} - \mu_i^{*} = 0 \<br> \alpha_i^{*} ((y_i(w^{*} \cdot x_i) + b^{*} - 1 + \xi_i^{*}) = 0 \<br> \mu_i^{*} \xi_i^{*} = 0 \<br> \alpha_i^{*} \ge 0 \<br> y_i(w^{*} \cdot x_i) + b^{*} - 1 + \xi_i^{*} \ge 0 \<br> \mu_i^{*} \ge 0 \<br> \xi_i^{*} = 0 \<br>\end{aligned}<br>$$<br>得到$ w^{*} = \sum_{i=1}^N \alpha_i^{*} y_i x_i$。若存在$\alpha_j^{*}, 0 &lt; \alpha_j^{*} &lt; C$，则 $y_j(w^{*} \cdot x_j) + b^{*} - 1 = 0$。从而得到$b^{*} = y_j - \sum_{i = 1}^N y_i\alpha_i^{*}(x_i\cdot x_j)$</p>
<p>分离超平面可以写成<br>$$<br>\sum_{i = 1}^N y_i\alpha_i^{*}(x\cdot x_i) + b^{*} = 0<br>$$<br>分类决策函数写成<br>$$<br>f(x) = \textrm{sign} (\sum_{i = 1}^N y_i\alpha_i^{*}(x\cdot x_i) + b^{*})<br>$$<br>$\alpha_i^{*}&gt;0$的实例称为软间隔的支持向量。若$\alpha_i^{*} &lt; C$，则$\xi_i = 0$，支持向量恰好落在间隔边界上；当$\alpha_i^{*} = C$时，若$0 &lt; \xi_i &lt;1$，则分类正确，实例在间隔边界和分离超平面之间，若$\xi_i = 1$，实例在分离超平面上，若$\xi_i &gt; 1$，则实例在分离超平面误分类的一侧。</p>
<p>线性SVM原始最优化问题等价于<br>$$<br>\min_{w, b} \sum_{i=1}^N \max(1 - y_i(w\cdot x_1 + b), 0) + \lambda|w|^2<br>$$</p>
<h1 id="非线性支持向量机-amp-核函数"><a href="#非线性支持向量机-amp-核函数" class="headerlink" title="非线性支持向量机&amp;核函数"></a>非线性支持向量机&amp;核函数</h1><h2 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h2><p>通过非线性变换将原空间的数据映射到新空间，在新空间里用线性分类学习方法从训练数据里学习分类模型。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>如果存在一个从输入空间$\mathcal{X}$到特征空间$\mathcal{H}$的映射$\phi(x): \mathcal{X}\to \mathcal{H}$使所有$x, z \in \mathcal{X}$，函数$K(x, z)$满足条件$K(x, z) = \phi(x) \cdot \phi(z)$，则称$K(x, z)$为核函数。</p>
<p>$K(x, z)$为正定核函数的充要条件为对任意$x_i \in \mathcal{X}, i = 1, \dots, m$，$K(x, z)$对应的Gram矩阵$K = [K(x_i, x_j)]_{m\times m}$是半正定矩阵。</p>
<p>常用核函数：</p>
<p>多项式核函数：$K(x, z) = (x\cdot z + 1)^p$</p>
<p>高斯核函数：$K(x, z) = \exp(- \frac{|x - z|^2}{2\sigma^2})$</p>
<p>字符串核函数：</p>
<p>考虑字符串$s$的子串$u$，给定指标序列$i$，$u = s(i)$，第一个字符到最后一个字符的距离为$l(i)$。$R^{\Sigma^n}$表示定义在所有长度为$n$的字符串的集合上的实数空间，每一维对应一个字符串$u \in \Sigma^n$，映射$\phi_n(s)$将字符串$s$映射$R^{\Sigma^n}$上的一个向量，其在$u$维上的取值为$\lbrack \phi_{n} (s)\rbrack_u = \sum_{i:s(i) = u}\lambda^{l(i)}$。最终得到核函数<br>$$<br>k_n(s, t) = \sum_{u \in \Sigma^n} \lbrack  \phi_n(s)\rbrack_u \lbrack  \phi_n(t)\rbrack_u =  \sum_{u \in \Sigma^n} \sum_{(i, j):s(i) = t(j) = u} \lambda^{l(i)}   \lambda^{l(j)}<br>$$</p>
<h2 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h2><p>从非线性分类训练集，通过核函数与软间隔最大化，学习到的分类决策函数<br>$$<br>f(x) = \textrm{sign}(\sum_{i=1}^N \alpha_i^{*} y_i K(x, x_i) + b^{*})<br>$$<br>称为非线性支持向量机。</p>
<h1 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h1><p>不断地将原来的二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量所有变量满足KKT条件。</p>
</div><div class="tags"></div><div class="post-nav"><a class="pre" href="/2020/08/03/Stanford-Compilers-%E7%AC%94%E8%AE%B0/">Stanford Compilers 笔记</a><a class="next" href="/2020/03/29/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B04-%E5%86%B3%E7%AD%96%E6%A0%91/">统计学习方法笔记4 决策树</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://luciusssss.github.io/2020/04/12/统计学习方法笔记5-SVM/';
    this.page.identifier = '2020/04/12/统计学习方法笔记5-SVM/';
    this.page.title = '统计学习方法笔记5 SVM';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//lucius.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//lucius.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://lucius.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://luciusssss.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91/">编译</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a> <a href="/tags/%E4%BA%8C%E5%88%86/" style="font-size: 15px;">二分</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size: 15px;">队列</a> <a href="/tags/%E6%A0%88/" style="font-size: 15px;">栈</a> <a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 15px;">论文</a> <a href="/tags/QA/" style="font-size: 15px;">QA</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 15px;">贪心</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E7%BC%96%E8%AF%91/" style="font-size: 15px;">编译</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/08/03/Stanford-Compilers-%E7%AC%94%E8%AE%B0/">Stanford Compilers 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B05-SVM/">统计学习方法笔记5 SVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/29/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B04-%E5%86%B3%E7%AD%96%E6%A0%91/">统计学习方法笔记4 决策树</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B03-k%E8%BF%91%E9%82%BB/">统计学习方法笔记3 k近邻</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B02-%E6%84%9F%E7%9F%A5%E6%9C%BA/">统计学习方法笔记2 感知机</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B01-%E6%A6%82%E8%AE%BA/">统计学习方法笔记1 概论</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/20/LeetCode%E4%BA%8C%E5%88%86-%E9%98%9F%E5%88%97-%E6%A0%88%E4%B8%93%E9%A2%98/">LeetCode二分/队列/栈专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/LeetCode%E6%A0%91%E4%B8%93%E9%A2%98/">LeetCode树专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/LeetCode%E8%B4%AA%E5%BF%83%E4%B8%93%E9%A2%98/">LeetCode贪心专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/28/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-2/">LeetCode刷题笔记 2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//lucius.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/luciusssss" title="My Github" target="_blank">My Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">Blog - Lucius.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>