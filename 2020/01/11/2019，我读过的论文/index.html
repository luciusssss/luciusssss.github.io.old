<!DOCTYPE html><html lang="zh-Hans"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Per aspera ad astra."><title>2019，我读过的论文 | Blog - Lucius</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-174277081-1','auto');ga('send','pageview');
</script><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">2019，我读过的论文</h1><a id="logo" href="/.">Blog - Lucius</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">2019，我读过的论文</h1><div class="post-meta">2020-01-11<span> | </span><span class="category"><a href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><a class="disqus-comment-count" data-disqus-identifier="2020/01/11/2019，我读过的论文/" href="/2020/01/11/2019%EF%BC%8C%E6%88%91%E8%AF%BB%E8%BF%87%E7%9A%84%E8%AE%BA%E6%96%87/#disqus_thread"></a><div class="post-content"><p>去年开始认真读论文还是从7月开始，主要读了很多Question Answering的论文，其中又以Machine Reading Comprehension为主，大概了解了QA这个领域在做些什么。<br>这里我把去年读过的论文简单列一下。其中我觉得值得一读的论文我会打上😻。<br>阅读过程中尤其感谢LAI Yuxuan师兄的指导！</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><ul>
<li><p>NEURAL READING COMPREHENSION AND BEYOND 😻<br><em>Chen Danqi</em><br>陈丹琦的博士毕业论文，一篇对MRC比较全面且易懂的综述。</p>
</li>
<li><p>Neural Machine Reading Comprehension: Methods and Trends<br><em>Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, Weiming Zhang</em><br>国防科大的MRC综述，一些想法很大胆，但一些地方没有讲明白。</p>
</li>
<li><p>Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches 😻<br><em>Shane Storks, Qiaozi Gao, Joyce Y. Chai</em><br>对于（广义的）Commonsense QA做的综述。</p>
</li>
</ul>
<a id="more"></a>


<h2 id="Machine-Reading-Comprehension-Dataset"><a href="#Machine-Reading-Comprehension-Dataset" class="headerlink" title="Machine Reading Comprehension Dataset"></a>Machine Reading Comprehension Dataset</h2><ul>
<li><p>SQuAD: 100,000+ Questions for Machine Comprehension of Text 😻<br><em>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang</em>, EMNLP 16<br>MRC领域最经典的数据集SQuAD。这是不含unanswerable question的1.0版本，问题来源于wikipedia，人工根据文章提问题。</p>
</li>
<li><p>Know What You Don’t Know: Unanswerable Questions for SQuAD 😻<br><em>Pranav Rajpurkar, Robin Jia, Percy Liang</em>, ACL 18<br>SQuAD 2.0，加入了大量unanswerable question</p>
</li>
<li><p>NewsQA: A Machine Comprehension Dataset<br><em>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman</em><br>文章来源于CNN，提问者只能看到新闻的summary points，答案从来自新闻正文，从而避免问题和文章过于相似。</p>
</li>
<li><p>ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension<br><em>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme</em><br>一个cloze形式的阅读理解数据集。文章是新闻的前几段，从新闻的剩余部分挑一个句子，挖掉句子中一个实体当作问题。</p>
</li>
<li><p>Natural Questions: A Benchmark for Question Answering Research 😻<br><em>Google</em>, TACL 19<br>以整个维基页面为文章，文章较长，同时标注长答案和短答案。</p>
</li>
<li><p>BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions<br><em>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova</em>, NAACL 19<br>研究自然产生的yes/no questions，制作了阅读理解数据集BoolQ，发现从entailment data迁移学习在它上面表现较好。</p>
</li>
<li><p>QUOREF: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning<br><em>Pradeep Dasigi, Nelson F. Liu, Ana Marasović, Noah A. Smith, Matt Gardner</em>, EMNLP 19<br>24k extractive MRC questions, 文章来自wiki(40%的文章是电影情节概要)，需要共指消解。</p>
</li>
<li><p>DuoRC: Towards complex language understanding with paraphrased reading comprehension<br><em>Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, Karthik Sankaranarayanan</em>, ACL 18<br>基于7680对电影情节概要(一篇来自维基，一篇来自imdb)。4 challenges: 很多问题与文章的lexical overlap低；需要background knowledge和commonsense knowledge；叙述性的文章经常需要综合多句的复杂推理；包含no answer的问题。</p>
</li>
<li><p>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications<br><em>Baidu</em>, ACL18<br>中文MRC数据集。</p>
</li>
</ul>
<h2 id="Commonsense-QA-Dataset"><a href="#Commonsense-QA-Dataset" class="headerlink" title="Commonsense QA Dataset"></a>Commonsense QA Dataset</h2><ul>
<li><p>COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge 😻<br><em>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant</em>, NAACL 19<br>基于ConceptNet做的选择题形式的常识问答。</p>
</li>
<li><p>COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning 😻<br><em>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</em>, EMNLP 19<br>阅读理解形式的选择题常识问答，需要通过commonsense inference来read between the lines。</p>
</li>
</ul>
<h2 id="Other-QA-Dataset"><a href="#Other-QA-Dataset" class="headerlink" title="Other QA Dataset"></a>Other QA Dataset</h2><ul>
<li><p>GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School<br><em>Zixian Huang, Yulin Shen, Xiao Li, Yuang Wei, Gong Cheng, Lin Zhou, Xinyu Dai, Yuzhong Qu</em>, EMNLP 19<br>高考地理题。如何处理题目中的图？</p>
</li>
<li><p>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms<br>37,200个数学题，附有选项和解题路径。</p>
</li>
</ul>
<h2 id="Dataset-Analysis-amp-Model-Analysis"><a href="#Dataset-Analysis-amp-Model-Analysis" class="headerlink" title="Dataset Analysis &amp; Model Analysis"></a>Dataset Analysis &amp; Model Analysis</h2><ul>
<li><p>What Makes Reading Comprehension Questions Easier?<br><em>Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa</em>, EMNLP 18<br>分析最近的12个MRC数据集，通过启发式规则分割数据集，然后检查各自的表现，认为<em>hard questions require knowledge inference and multiple-sentence reasoning</em>。</p>
</li>
<li><p>Adversarial Examples for Evaluating Reading Comprehension Systems 😻<br><em>Robin Jia, Percy Liang</em>, EMNLP 19<br>对MRC进行反思的比较早的一篇。通过在文章结尾加上对抗性的句子能让模型回答错问题。</p>
</li>
<li><p>Do NLP Models Know Numbers? Probing Numeracy in Embeddings 😻<br><em>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner</em>, EMNLP 19<br>研究发现state-of-the-art的QA模型在DROP上也表现出很好的numerical reasoning能力。为了理解这种能力如何产生，作者探索了各种embedding方法在list maximum, number encoding, addition三个任务上的表现。发现standard embeddings中存在着很大程度的numeracy，character-level embeddings对numeracy的表示很精确。</p>
</li>
</ul>
<h2 id="QA-Model"><a href="#QA-Model" class="headerlink" title="QA Model"></a>QA Model</h2><ul>
<li><p>Bidirectional Attention Flow for Machine Comprehension 😻<br><em>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi</em>, ICLR 17<br>基于文章和问题双向注意力的BiDAF模型。是MRC中基于注意力机制最经典的模型。</p>
</li>
<li><p>Latent Retrieval for Weakly Supervised Open Domain Question Answering<br><em>Kenton Lee, Ming-Wei Chang, Kristina Toutanova</em>, ACL 19<br>提了一个Open Domain QA的端到端模型。</p>
<p><strong>下面是对于Conversational MRC的方法：</strong></p>
</li>
<li><p>GRAPHFLOW: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension<br><em>Yu Chen, Lingfei Wu, Mohammed J. Zaki</em><br>做conversational MRC的，捕获对话中的conversational flow：将conversational flow建模成对话中的一系列latent states。</p>
</li>
<li><p>Answering Conversational Questions on Structured Data without Logical Forms<br><em>Thomas Müller, Francesco Piccinno, Massimo Nicosia, Peter Shaw, Yasemin Altun</em>, EMNLP 19<br>搜集了 包含6k个对于维基上的半结构化表格进行询问的问题序列 的数据集，并提出了一个无需logical froms的weakly supervised semantic parsing模型。</p>
</li>
<li><p>Technical report on Conversational Question Answering<br>在CoQA上使用RoBERTa + AT(Adversarial Training) + KD(Knowledge Distillation)，达到90.4 F1。</p>
<p><strong>下面是对于Multi-hop MRC的方法：</strong></p>
</li>
<li><p>Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension<br><em>Yimeng Zhuang, Huadong Wang</em>, ACL 19<br>在token-level处理cross-passage infomation，能够动态地从序列中选取重要的token，达到speed, memory和accuracy的平衡。</p>
</li>
<li><p>Answering Complex Open-domain Questions Through Iterative Query Generation<br><em>Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, Christopher D. Manning</em>, EMNLP 19<br>在每一步，利用之前hop的IR结果生成一个新的自然语言query，用off-the-shelf的IR系统去retrive新的证据来回答问题。</p>
</li>
<li><p>Cognitive Graph for Multi-Hop Reading Comprehension at Scale<br><em>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang</em>, ACL 19<br>根据心理学中的Dual process theory，大脑先无意识地、隐性地、直觉地利用注意力检索相关信息(system 1)，然后有意识地、显性地、可控地进行推理(system 2)。</p>
<p><strong>下面是对于Commonsense QA的方法和一些引入External Knowledge的方法：</strong></p>
</li>
<li><p>Augmenting Neural Networks with First-order Logic<br><em>Tao Li, Vivek Srikumar</em>, ACL19<br>直接把一阶逻辑表示的外部知识（这里用了ConceptNet）嵌入到神经网络架构中，而仍然保持端到端的训练方式，同时依靠这些外部规则来减少对数据的依赖。</p>
</li>
<li><p>Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models<br><em>Zhi-Xiu Ye, Qian Chen, Wen Wang, Zhen-Hua Ling</em><br>现存预训练的语言表示模型很少考虑直接将常识知识嵌入。本文用“align, mask, and select”的方法构造有关常识的数据集，并在它上面预训练BERT，从而将常识知识引入到语言表示模型。</p>
</li>
<li><p>Explicit Utilization of General Knowledge in Machine Reading Comprehension 😻<br><em>Chao Wang, Hui Jiang</em>, ACL 19<br>现有MRC模型与人类之间的gap体现在对数据的需求和抵御噪声的鲁棒性。为了缓解这两个问题，作者使用WordNet从passage-question pair中抽取词间语义关系作为general knowledge，来辅助端到端模型中的注意力机制。</p>
</li>
<li><p>Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension<br><em>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, Sujian Li</em>, ACL 19<br>使用WordNet和NELL两个KB作为MRC的外部知识，通过注意力机制选取需要的知识注入到BERT中，在Record和squad1.1上取得了很好的效果。</p>
</li>
<li><p>Careful Selection of Knowledge to solve Open Book Question Answering<br><em>Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral</em><br>用比较IR的方法做OpenBookQA。</p>
</li>
<li><p>KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning 😻<br>一种类似DBQA的方法，利用ConceptNet建图做COMMONSENSEQA数据集。</p>
<p><strong>下面是对于Discrete Reasoning和Multi-span Extraction的方法（主要是针对DROP数据集）：</strong><br>DROP数据集答案包含多种类型：data、number、text span(s)，需要numerical operations比如adding、 sorting、counting。</p>
</li>
<li><p>A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning<br><em>Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li</em>, EMNLP 19</p>
</li>
<li><p>Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension 😻<br><em>Google</em>, EMNLP 19</p>
</li>
<li><p>A Discrete Hard EM Approach for Weakly Supervised Question Answering<br><em>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer</em>, EMNLP 19</p>
</li>
<li><p>Tag-based Multi-Span Extraction in Reading Comprehension<br><em>Avia Efrat, Elad Segal, Mor Shoham</em></p>
<p><strong>下面是处理Unanswerable Questions的方法：</strong> </p>
</li>
<li><p>Relation Module for Non-answerable Prediction on Reading Comprehension<br><em>DiDi</em><br>通过增加relation module来提高MRC模型判断问题是否有答案的能力。</p>
</li>
</ul>
<h2 id="Transfer-Learning-amp-Multi-task-Learning"><a href="#Transfer-Learning-amp-Multi-task-Learning" class="headerlink" title="Transfer Learning &amp; Multi-task Learning"></a>Transfer Learning &amp; Multi-task Learning</h2><ul>
<li><p>MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension 😻<br><em>Alon Talmor, Jonathan Berant</em>, ACL 19<br>研究各个MRC数据集能否互相泛化。实验做了很多。</p>
</li>
<li><p>Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension<br><em>Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao</em>, NAACL 19<br>通过将多个任务的数据集使用精细的采样结合在一起训练，提高模型的泛化性。</p>
</li>
<li><p>Unsupervised Domain Adaptation on Reading Comprehension<br>AAAI 20</p>
</li>
</ul>
<h2 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h2><ul>
<li>Learning to Ask Unanswerable Questions for Machine Reading Comprehension 😻<br><em>Haichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing Qin, Ting Liu</em>, ACL 19<br>通过一种pair2sequence的方式，由可回答问题和段落生成不可回答问题。生成的不可回答问题当作data augmentation提升了BERT的表现。</li>
</ul>
<h2 id="Text-Matching"><a href="#Text-Matching" class="headerlink" title="Text Matching"></a>Text Matching</h2><ul>
<li><p>MIX: Multi-Channel Information Crossing for Text Matching<br><em>Tencent</em>, KDD 18<br>从多种细粒度进行文本匹配。</p>
</li>
<li><p>Enhanced LSTM for Natural Language Inference 😻<br><em>Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen</em>, ACL 17<br>一个基于BiLSTM + Attention的很有效的文本匹配模型ESIM。</p>
</li>
</ul>
<h2 id="NLI-Dataset"><a href="#NLI-Dataset" class="headerlink" title="NLI Dataset"></a>NLI Dataset</h2><ul>
<li>Adversarial NLI: A New Benchmark for Natural Language Understanding 😻<br><em>Facebook</em><br>通过迭代的、对抗性的人类-模型循环，得到了一个大规模的NLI数据集。在这个新数据集上训练的模型在很多NLI数据集上达到state-of-the-art表现。这个数据集也说明了非专家标注者也能成功找到模型们的缺点。数据收集的方法可以运用于never-ending learning，可以成为一个moving target，而非一个快速饱和的静态benchmark。总得来说，解决了现存数据集benchmark longevity和robustness两个问题。</li>
</ul>
<h2 id="Story-Ending-Prediction"><a href="#Story-Ending-Prediction" class="headerlink" title="Story Ending Prediction"></a>Story Ending Prediction</h2><p>都是做Story Cloze Test这个数据集的。</p>
<ul>
<li>LSDSem 2017: Exploring Data Generation Methods for the Story Cloze Test<br>用了一个feature-based的方法，效果还挺好</li>
<li>Improving Language Understanding by Generative Pre-Training</li>
<li>A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task<br>用了SemLM</li>
<li>An RNN-based Binary Classifier for the Story Cloze Test<br>用了Skip-thought</li>
<li>A Simple and Effective Approach to the Story Cloze Test<br>用了Skip-thought</li>
<li>Story Ending Selection by Finding Hints from Pairwise Candidate Endings<br>隔壁的工作，将两个选项同时放入模型进行区分</li>
<li>Find a Reasonable Ending for Stories: Does Logic Relation Help the Story Cloze Test? 😻<br>AAAI 19。用NLI数据预训练，用逻辑知识辅助判断。</li>
<li>Narrative Modeling with Memory Chains and Semantic Supervision</li>
<li>Discriminative Sentence Modeling for Story Ending Prediction<br>AAAI 20</li>
</ul>
</div><div class="tags"><a href="/tags/%E8%AE%BA%E6%96%87/"><i class="fa fa-tag"></i>论文</a><a href="/tags/QA/"><i class="fa fa-tag"></i>QA</a></div><div class="post-nav"><a class="pre" href="/2020/01/20/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-1/">LeetCode刷题笔记 1</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://luciusssss.github.io/2020/01/11/2019，我读过的论文/';
    this.page.identifier = '2020/01/11/2019，我读过的论文/';
    this.page.title = '2019，我读过的论文';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//lucius.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//lucius.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://lucius.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://luciusssss.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BC%96%E8%AF%91/">编译</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/LeetCode/" style="font-size: 15px;">LeetCode</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 15px;">数据结构</a> <a href="/tags/%E4%BA%8C%E5%88%86/" style="font-size: 15px;">二分</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size: 15px;">队列</a> <a href="/tags/%E6%A0%88/" style="font-size: 15px;">栈</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 15px;">论文</a> <a href="/tags/QA/" style="font-size: 15px;">QA</a> <a href="/tags/%E7%BC%96%E8%AF%91/" style="font-size: 15px;">编译</a> <a href="/tags/%E6%A0%91/" style="font-size: 15px;">树</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 15px;">统计学习方法</a> <a href="/tags/%E8%B4%AA%E5%BF%83/" style="font-size: 15px;">贪心</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/08/03/Stanford-Compilers-%E7%AC%94%E8%AE%B0/">Stanford Compilers 笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/04/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B05-SVM/">统计学习方法笔记5 SVM</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/29/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B04-%E5%86%B3%E7%AD%96%E6%A0%91/">统计学习方法笔记4 决策树</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B03-k%E8%BF%91%E9%82%BB/">统计学习方法笔记3 k近邻</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B02-%E6%84%9F%E7%9F%A5%E6%9C%BA/">统计学习方法笔记2 感知机</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/27/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AC%94%E8%AE%B01-%E6%A6%82%E8%AE%BA/">统计学习方法笔记1 概论</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/20/LeetCode%E4%BA%8C%E5%88%86-%E9%98%9F%E5%88%97-%E6%A0%88%E4%B8%93%E9%A2%98/">LeetCode二分/队列/栈专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/LeetCode%E6%A0%91%E4%B8%93%E9%A2%98/">LeetCode树专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/06/LeetCode%E8%B4%AA%E5%BF%83%E4%B8%93%E9%A2%98/">LeetCode贪心专题</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/28/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-2/">LeetCode刷题笔记 2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//lucius.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://github.com/luciusssss" title="My Github" target="_blank">My Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">Blog - Lucius.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>