<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog - Lucius</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://luciusssss.github.io/"/>
  <updated>2020-01-22T09:52:14.344Z</updated>
  <id>http://luciusssss.github.io/</id>
  
  <author>
    <name>Lucius</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LeetCode刷题笔记</title>
    <link href="http://luciusssss.github.io/2020/01/20/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
    <id>http://luciusssss.github.io/2020/01/20/LeetCode%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/</id>
    <published>2020-01-20T04:45:55.000Z</published>
    <updated>2020-01-22T09:52:14.344Z</updated>
    
    <content type="html"><![CDATA[<p>开始刷Top 100 Liked Questions啦！</p><a id="more"></a><h1 id="1-Two-Sum"><a href="#1-Two-Sum" class="headerlink" title="1. Two Sum"></a>1. Two Sum</h1><p>用unordered_map。O(n)</p><h1 id="2-Add-Two-Numbers"><a href="#2-Add-Two-Numbers" class="headerlink" title="2. Add Two Numbers"></a>2. Add Two Numbers</h1><p>同时遍历两个链表</p><h1 id="3-Longest-Substring-Without-Repeating-Characters"><a href="#3-Longest-Substring-Without-Repeating-Characters" class="headerlink" title="3. Longest Substring Without Repeating Characters"></a>3. Longest Substring Without Repeating Characters</h1><p>DP。计算以第i位字符位结尾的满足要求的字符串长度。$O(n^2)$。<br>注意edge case: 长度为0字符串，长度为1字符串之类的。。。</p><h1 id="4-Median-of-Two-Sorted-Arrays-🌟"><a href="#4-Median-of-Two-Sorted-Arrays-🌟" class="headerlink" title="4. Median of Two Sorted Arrays 🌟"></a>4. Median of Two Sorted Arrays 🌟</h1><p>第一道hard题，解析见<a href="https://leetcode.com/articles/median-of-two-sorted-arrays/" target="_blank" rel="noopener">https://leetcode.com/articles/median-of-two-sorted-arrays/</a><br>复杂度为$O(\log min(m, n))$.</p><h1 id="5-Longest-Palindromic-Substring"><a href="#5-Longest-Palindromic-Substring" class="headerlink" title="5. Longest Palindromic Substring"></a>5. Longest Palindromic Substring</h1><p>一开始想了一个O(n)的DP后来发现是错的，比如在“bananas”这个例子上。。。<br>中心扩展，分奇偶长度串。写扩展函数时，注意把string串引用比较好。时间$O(n^2)$，空间$O(n)$。<br>DP，时间$O(n^2)$，空间$O(n^2)$。<br>$O(n)$的算法，Manacher，类似KMP？之后再看。</p><h1 id="10-Regular-Expression-Matching-🌟"><a href="#10-Regular-Expression-Matching-🌟" class="headerlink" title="10. Regular Expression Matching 🌟"></a>10. Regular Expression Matching 🌟</h1><p>需要想一下的DP，O(mn)。<br>解析<a href="https://www.cnblogs.com/Jessey-Ge/p/10993447.html" target="_blank" rel="noopener">https://www.cnblogs.com/Jessey-Ge/p/10993447.html</a></p><h1 id="11-Container-With-Most-Water-🌟"><a href="#11-Container-With-Most-Water-🌟" class="headerlink" title="11. Container With Most Water 🌟"></a>11. Container With Most Water 🌟</h1><p>O(n)的方法：双指针，每次将较短的那个向内移动。</p><h1 id="15-3Sum"><a href="#15-3Sum" class="headerlink" title="15. 3Sum"></a>15. 3Sum</h1><p>类似Two Sum。先排序$O(n\log n)$。固定target，然后双指针找Two Sum。总共$O(n^2)$。<br><strong>尤其注意两层循环都要把重复的去除。</strong></p><h1 id="17-Letter-Combinations-of-a-Phone-Number"><a href="#17-Letter-Combinations-of-a-Phone-Number" class="headerlink" title="17. Letter Combinations of a Phone Number"></a>17. Letter Combinations of a Phone Number</h1><p>直接枚举。vector删除第一个元素可以用<code>ret.erase(ret.begin())</code>。<strong>尤其注意边界情况！！！当输入字符串为空要单独判断！！！</strong></p><h1 id="19-Remove-Nth-Node-From-End-of-List"><a href="#19-Remove-Nth-Node-From-End-of-List" class="headerlink" title="19. Remove Nth Node From End of List"></a>19. Remove Nth Node From End of List</h1><p>可以用双指针走一遍。 可以使用dummy指针当作链表头。</p><h1 id="20-Valid-Parentheses"><a href="#20-Valid-Parentheses" class="headerlink" title="20. Valid Parentheses"></a>20. Valid Parentheses</h1><p>stack。但我还是没有一次bug-free。。。取stack的top的时，一定要先检查是否为空！！！</p><h1 id="21-Merge-Two-Sorted-Lists"><a href="#21-Merge-Two-Sorted-Lists" class="headerlink" title="21. Merge Two Sorted Lists"></a>21. Merge Two Sorted Lists</h1><h1 id="22-Generate-Parentheses"><a href="#22-Generate-Parentheses" class="headerlink" title="22. Generate Parentheses"></a>22. Generate Parentheses</h1><p>回溯。一开始用了stack，其实不用。<code>leftCnt &lt; n</code>时尝试加上<code>(</code>，<code>rightCnt &lt; leftCnt</code>时尝试加上<code>)</code>。</p><h1 id="23-Merge-k-Sorted-Lists"><a href="#23-Merge-k-Sorted-Lists" class="headerlink" title="23. Merge k Sorted Lists"></a>23. Merge k Sorted Lists</h1><p>分治。时间$O(N\log k)$。注意<code>k==0</code>的情形。空间$O(1)$，两两合并可以参照<a href="https://www.geeksforgeeks.org/merge-k-sorted-linked-lists/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/merge-k-sorted-linked-lists/</a><br>优先队列，时间复杂度一样：<a href="https://blog.csdn.net/Ethan95/article/details/85195403" target="_blank" rel="noopener">https://blog.csdn.net/Ethan95/article/details/85195403</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//优先队列的使用：默认是从大到小排。自定义时需要重载&lt;符号</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">cmp</span>&#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(Node a, Node b)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.x == b.x)<span class="keyword">return</span> a.y&gt;b.y;</span><br><span class="line">        <span class="keyword">return</span> a.x&gt;b.x;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line">priority_queue&lt;Node, <span class="built_in">vector</span>&lt;Node&gt;, cmp&gt;p;</span><br></pre></td></tr></table></figure><h1 id="31-Next-Permutation-🌟"><a href="#31-Next-Permutation-🌟" class="headerlink" title="31. Next Permutation 🌟"></a>31. Next Permutation 🌟</h1><p>有非常巧妙的时间$O(n)$，空间$O(1)$的做法：<a href="https://leetcode.com/articles/next-permutation/" target="_blank" rel="noopener">https://leetcode.com/articles/next-permutation/</a><br>首先从结尾开始找到最长的不升子序列，然后将该序列前一位数换成子序列中刚好比它大一点的数，在将子序列reverse一下。</p><h1 id="32-Longest-Valid-Parentheses-🌟"><a href="#32-Longest-Valid-Parentheses-🌟" class="headerlink" title="32. Longest Valid Parentheses 🌟"></a>32. Longest Valid Parentheses 🌟</h1><p>一道hard的DP。时间空间都是$O(n)$。解析：<a href="https://leetcode.com/articles/longest-valid-parentheses/" target="_blank" rel="noopener">https://leetcode.com/articles/longest-valid-parentheses/</a>  若当前为<code>)</code>则可能存在以当前位为结尾的有效串，分<code>))</code>和<code>()</code>讨论。<br>也可以用栈，复杂度相同。更清晰的解释：<a href="https://leetcode.wang/leetCode-32-Longest-Valid-Parentheses.html" target="_blank" rel="noopener">https://leetcode.wang/leetCode-32-Longest-Valid-Parentheses.html</a><br>时间$O(n)$的方法，可以从22题Generate Parentheses找到影子。<strong>注意要从左到右、从右到左各扫描一遍。</strong></p><h1 id="33-Search-in-Rotated-Sorted-Array-🌟"><a href="#33-Search-in-Rotated-Sorted-Array-🌟" class="headerlink" title="33. Search in Rotated Sorted Array 🌟"></a>33. Search in Rotated Sorted Array 🌟</h1><p>二分。<a href="https://www.cnblogs.com/zle1992/p/8996225.html" target="_blank" rel="noopener">https://www.cnblogs.com/zle1992/p/8996225.html</a></p><h1 id="34-Find-First-and-Last-Position-of-Element-in-Sorted-Array"><a href="#34-Find-First-and-Last-Position-of-Element-in-Sorted-Array" class="headerlink" title="34. Find First and Last Position of Element in Sorted Array"></a>34. Find First and Last Position of Element in Sorted Array</h1><p>二分。最直观的方法可以三次二分。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开始刷Top 100 Liked Questions啦！&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://luciusssss.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="leetcode" scheme="http://luciusssss.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>2019，我读过的论文</title>
    <link href="http://luciusssss.github.io/2020/01/11/2019%EF%BC%8C%E6%88%91%E8%AF%BB%E8%BF%87%E7%9A%84%E8%AE%BA%E6%96%87/"/>
    <id>http://luciusssss.github.io/2020/01/11/2019%EF%BC%8C%E6%88%91%E8%AF%BB%E8%BF%87%E7%9A%84%E8%AE%BA%E6%96%87/</id>
    <published>2020-01-11T07:00:13.000Z</published>
    <updated>2020-01-21T10:40:44.674Z</updated>
    
    <content type="html"><![CDATA[<p>去年开始认真读论文还是从7月开始，主要读了很多Question Answering的论文，其中又以Machine Reading Comprehension为主，大概了解了QA这个领域在做些什么。<br>这里我把去年读过的论文简单列一下。其中我觉得值得一读的论文我会打上😻。<br>阅读过程中尤其感谢LAI Yuxuan师兄的指导！</p><h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><ul><li><p>NEURAL READING COMPREHENSION AND BEYOND 😻<br><em>Chen Danqi</em><br>陈丹琦的博士毕业论文，一篇对MRC比较全面且易懂的综述。</p></li><li><p>Neural Machine Reading Comprehension: Methods and Trends<br><em>Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, Weiming Zhang</em><br>国防科大的MRC综述，一些想法很大胆，但一些地方没有讲明白。</p></li><li><p>Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches 😻<br><em>Shane Storks, Qiaozi Gao, Joyce Y. Chai</em><br>对于（广义的）Commonsense QA做的综述。</p></li></ul><a id="more"></a><h2 id="Machine-Reading-Comprehension-Dataset"><a href="#Machine-Reading-Comprehension-Dataset" class="headerlink" title="Machine Reading Comprehension Dataset"></a>Machine Reading Comprehension Dataset</h2><ul><li><p>SQuAD: 100,000+ Questions for Machine Comprehension of Text 😻<br><em>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang</em>, EMNLP 16<br>MRC领域最经典的数据集SQuAD。这是不含unanswerable question的1.0版本，问题来源于wikipedia，人工根据文章提问题。</p></li><li><p>Know What You Don’t Know: Unanswerable Questions for SQuAD 😻<br><em>Pranav Rajpurkar, Robin Jia, Percy Liang</em>, ACL 18<br>SQuAD 2.0，加入了大量unanswerable question</p></li><li><p>NewsQA: A Machine Comprehension Dataset<br><em>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman</em><br>文章来源于CNN，提问者只能看到新闻的summary points，答案从来自新闻正文，从而避免问题和文章过于相似。</p></li><li><p>ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension<br><em>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme</em><br>一个cloze形式的阅读理解数据集。文章是新闻的前几段，从新闻的剩余部分挑一个句子，挖掉句子中一个实体当作问题。</p></li><li><p>Natural Questions: A Benchmark for Question Answering Research 😻<br><em>Google</em>, TACL 19<br>以整个维基页面为文章，文章较长，同时标注长答案和短答案。</p></li><li><p>BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions<br><em>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova</em>, NAACL 19<br>研究自然产生的yes/no questions，制作了阅读理解数据集BoolQ，发现从entailment data迁移学习在它上面表现较好。</p></li><li><p>QUOREF: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning<br><em>Pradeep Dasigi, Nelson F. Liu, Ana Marasović, Noah A. Smith, Matt Gardner</em>, EMNLP 19<br>24k extractive MRC questions, 文章来自wiki(40%的文章是电影情节概要)，需要共指消解。</p></li><li><p>DuoRC: Towards complex language understanding with paraphrased reading comprehension<br><em>Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, Karthik Sankaranarayanan</em>, ACL 18<br>基于7680对电影情节概要(一篇来自维基，一篇来自imdb)。4 challenges: 很多问题与文章的lexical overlap低；需要background knowledge和commonsense knowledge；叙述性的文章经常需要综合多句的复杂推理；包含no answer的问题。</p></li><li><p>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications<br><em>Baidu</em>, ACL18<br>中文MRC数据集。</p></li></ul><h2 id="Commonsense-QA-Dataset"><a href="#Commonsense-QA-Dataset" class="headerlink" title="Commonsense QA Dataset"></a>Commonsense QA Dataset</h2><ul><li><p>COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge 😻<br><em>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant</em>, NAACL 19<br>基于ConceptNet做的选择题形式的常识问答。</p></li><li><p>COSMOS QA: Machine Reading Comprehension with Contextual Commonsense Reasoning 😻<br><em>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi</em>, EMNLP 19<br>阅读理解形式的选择题常识问答，需要通过commonsense inference来read between the lines。</p></li></ul><h2 id="Other-QA-Dataset"><a href="#Other-QA-Dataset" class="headerlink" title="Other QA Dataset"></a>Other QA Dataset</h2><ul><li><p>GeoSQA: A Benchmark for Scenario-based Question Answering in the Geography Domain at High School<br><em>Zixian Huang, Yulin Shen, Xiao Li, Yuang Wei, Gong Cheng, Lin Zhou, Xinyu Dai, Yuzhong Qu</em>, EMNLP 19<br>高考地理题。如何处理题目中的图？</p></li><li><p>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms<br>37,200个数学题，附有选项和解题路径。</p></li></ul><h2 id="Dataset-Analysis-amp-Model-Analysis"><a href="#Dataset-Analysis-amp-Model-Analysis" class="headerlink" title="Dataset Analysis &amp; Model Analysis"></a>Dataset Analysis &amp; Model Analysis</h2><ul><li><p>What Makes Reading Comprehension Questions Easier?<br><em>Saku Sugawara, Kentaro Inui, Satoshi Sekine, Akiko Aizawa</em>, EMNLP 18<br>分析最近的12个MRC数据集，通过启发式规则分割数据集，然后检查各自的表现，认为<em>hard questions require knowledge inference and multiple-sentence reasoning</em>。</p></li><li><p>Adversarial Examples for Evaluating Reading Comprehension Systems 😻<br><em>Robin Jia, Percy Liang</em>, EMNLP 19<br>对MRC进行反思的比较早的一篇。通过在文章结尾加上对抗性的句子能让模型回答错问题。</p></li><li><p>Do NLP Models Know Numbers? Probing Numeracy in Embeddings 😻<br><em>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner</em>, EMNLP 19<br>研究发现state-of-the-art的QA模型在DROP上也表现出很好的numerical reasoning能力。为了理解这种能力如何产生，作者探索了各种embedding方法在list maximum, number encoding, addition三个任务上的表现。发现standard embeddings中存在着很大程度的numeracy，character-level embeddings对numeracy的表示很精确。</p></li></ul><h2 id="QA-Model"><a href="#QA-Model" class="headerlink" title="QA Model"></a>QA Model</h2><ul><li><p>Bidirectional Attention Flow for Machine Comprehension 😻<br><em>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi</em>, ICLR 17<br>基于文章和问题双向注意力的BiDAF模型。是MRC中基于注意力机制最经典的模型。</p></li><li><p>Latent Retrieval for Weakly Supervised Open Domain Question Answering<br><em>Kenton Lee, Ming-Wei Chang, Kristina Toutanova</em>, ACL 19<br>提了一个Open Domain QA的端到端模型。</p><p><strong>下面是对于Conversational MRC的方法：</strong></p></li><li><p>GRAPHFLOW: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension<br><em>Yu Chen, Lingfei Wu, Mohammed J. Zaki</em><br>做conversational MRC的，捕获对话中的conversational flow：将conversational flow建模成对话中的一系列latent states。</p></li><li><p>Answering Conversational Questions on Structured Data without Logical Forms<br><em>Thomas Müller, Francesco Piccinno, Massimo Nicosia, Peter Shaw, Yasemin Altun</em>, EMNLP 19<br>搜集了 包含6k个对于维基上的半结构化表格进行询问的问题序列 的数据集，并提出了一个无需logical froms的weakly supervised semantic parsing模型。</p></li><li><p>Technical report on Conversational Question Answering<br>在CoQA上使用RoBERTa + AT(Adversarial Training) + KD(Knowledge Distillation)，达到90.4 F1。</p><p><strong>下面是对于Multi-hop MRC的方法：</strong></p></li><li><p>Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension<br><em>Yimeng Zhuang, Huadong Wang</em>, ACL 19<br>在token-level处理cross-passage infomation，能够动态地从序列中选取重要的token，达到speed, memory和accuracy的平衡。</p></li><li><p>Answering Complex Open-domain Questions Through Iterative Query Generation<br><em>Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, Christopher D. Manning</em>, EMNLP 19<br>在每一步，利用之前hop的IR结果生成一个新的自然语言query，用off-the-shelf的IR系统去retrive新的证据来回答问题。</p></li><li><p>Cognitive Graph for Multi-Hop Reading Comprehension at Scale<br><em>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang</em>, ACL 19<br>根据心理学中的Dual process theory，大脑先无意识地、隐性地、直觉地利用注意力检索相关信息(system 1)，然后有意识地、显性地、可控地进行推理(system 2)。</p><p><strong>下面是对于Commonsense QA的方法和一些引入External Knowledge的方法：</strong></p></li><li><p>Augmenting Neural Networks with First-order Logic<br><em>Tao Li, Vivek Srikumar</em>, ACL19<br>直接把一阶逻辑表示的外部知识（这里用了ConceptNet）嵌入到神经网络架构中，而仍然保持端到端的训练方式，同时依靠这些外部规则来减少对数据的依赖。</p></li><li><p>Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models<br><em>Zhi-Xiu Ye, Qian Chen, Wen Wang, Zhen-Hua Ling</em><br>现存预训练的语言表示模型很少考虑直接将常识知识嵌入。本文用“align, mask, and select”的方法构造有关常识的数据集，并在它上面预训练BERT，从而将常识知识引入到语言表示模型。</p></li><li><p>Explicit Utilization of General Knowledge in Machine Reading Comprehension 😻<br><em>Chao Wang, Hui Jiang</em>, ACL 19<br>现有MRC模型与人类之间的gap体现在对数据的需求和抵御噪声的鲁棒性。为了缓解这两个问题，作者使用WordNet从passage-question pair中抽取词间语义关系作为general knowledge，来辅助端到端模型中的注意力机制。</p></li><li><p>Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension<br><em>An Yang, Quan Wang, Jing Liu, Kai Liu, Yajuan Lyu, Hua Wu, Qiaoqiao She, Sujian Li</em>, ACL 19<br>使用WordNet和NELL两个KB作为MRC的外部知识，通过注意力机制选取需要的知识注入到BERT中，在Record和squad1.1上取得了很好的效果。</p></li><li><p>Careful Selection of Knowledge to solve Open Book Question Answering<br><em>Pratyay Banerjee, Kuntal Kumar Pal, Arindam Mitra, Chitta Baral</em><br>用比较IR的方法做OpenBookQA。</p></li><li><p>KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning 😻<br>一种类似DBQA的方法，利用ConceptNet建图做COMMONSENSEQA数据集。</p><p><strong>下面是对于Discrete Reasoning和Multi-span Extraction的方法（主要是针对DROP数据集）：</strong><br>DROP数据集答案包含多种类型：data、number、text span(s)，需要numerical operations比如adding、 sorting、counting。</p></li><li><p>A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning<br><em>Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li</em>, EMNLP 19</p></li><li><p>Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension 😻<br><em>Google</em>, EMNLP 19</p></li><li><p>A Discrete Hard EM Approach for Weakly Supervised Question Answering<br><em>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer</em>, EMNLP 19</p></li><li><p>Tag-based Multi-Span Extraction in Reading Comprehension<br><em>Avia Efrat, Elad Segal, Mor Shoham</em></p><p><strong>下面是处理Unanswerable Questions的方法：</strong> </p></li><li><p>Relation Module for Non-answerable Prediction on Reading Comprehension<br><em>DiDi</em><br>通过增加relation module来提高MRC模型判断问题是否有答案的能力。</p></li></ul><h2 id="Transfer-Learning-amp-Multi-task-Learning"><a href="#Transfer-Learning-amp-Multi-task-Learning" class="headerlink" title="Transfer Learning &amp; Multi-task Learning"></a>Transfer Learning &amp; Multi-task Learning</h2><ul><li><p>MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension 😻<br><em>Alon Talmor, Jonathan Berant</em>, ACL 19<br>研究各个MRC数据集能否互相泛化。实验做了很多。</p></li><li><p>Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension<br><em>Yichong Xu, Xiaodong Liu, Yelong Shen, Jingjing Liu, Jianfeng Gao</em>, NAACL 19<br>通过将多个任务的数据集使用精细的采样结合在一起训练，提高模型的泛化性。</p></li><li><p>Unsupervised Domain Adaptation on Reading Comprehension<br>AAAI 20</p></li></ul><h2 id="Question-Generation"><a href="#Question-Generation" class="headerlink" title="Question Generation"></a>Question Generation</h2><ul><li>Learning to Ask Unanswerable Questions for Machine Reading Comprehension 😻<br><em>Haichao Zhu, Li Dong, Furu Wei, Wenhui Wang, Bing Qin, Ting Liu</em>, ACL 19<br>通过一种pair2sequence的方式，由可回答问题和段落生成不可回答问题。生成的不可回答问题当作data augmentation提升了BERT的表现。</li></ul><h2 id="Text-Matching"><a href="#Text-Matching" class="headerlink" title="Text Matching"></a>Text Matching</h2><ul><li><p>MIX: Multi-Channel Information Crossing for Text Matching<br><em>Tencent</em>, KDD 18<br>从多种细粒度进行文本匹配。</p></li><li><p>Enhanced LSTM for Natural Language Inference 😻<br><em>Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, Diana Inkpen</em>, ACL 17<br>一个基于BiLSTM + Attention的很有效的文本匹配模型ESIM。</p></li></ul><h2 id="NLI-Dataset"><a href="#NLI-Dataset" class="headerlink" title="NLI Dataset"></a>NLI Dataset</h2><ul><li>Adversarial NLI: A New Benchmark for Natural Language Understanding 😻<br><em>Facebook</em><br>通过迭代的、对抗性的人类-模型循环，得到了一个大规模的NLI数据集。在这个新数据集上训练的模型在很多NLI数据集上达到state-of-the-art表现。这个数据集也说明了非专家标注者也能成功找到模型们的缺点。数据收集的方法可以运用于never-ending learning，可以成为一个moving target，而非一个快速饱和的静态benchmark。总得来说，解决了现存数据集benchmark longevity和robustness两个问题。</li></ul><h2 id="Story-Ending-Prediction"><a href="#Story-Ending-Prediction" class="headerlink" title="Story Ending Prediction"></a>Story Ending Prediction</h2><p>都是做Story Cloze Test这个数据集的。</p><ul><li>LSDSem 2017: Exploring Data Generation Methods for the Story Cloze Test<br>用了一个feature-based的方法，效果还挺好</li><li>Improving Language Understanding by Generative Pre-Training</li><li>A Multi-Attention based Neural Network with External Knowledge for Story Ending Predicting Task<br>用了SemLM</li><li>An RNN-based Binary Classifier for the Story Cloze Test<br>用了Skip-thought</li><li>A Simple and Effective Approach to the Story Cloze Test<br>用了Skip-thought</li><li>Story Ending Selection by Finding Hints from Pairwise Candidate Endings<br>隔壁的工作，将两个选项同时放入模型进行区分</li><li>Find a Reasonable Ending for Stories: Does Logic Relation Help the Story Cloze Test? 😻<br>AAAI 19。用NLI数据预训练，用逻辑知识辅助判断。</li><li>Narrative Modeling with Memory Chains and Semantic Supervision</li><li>Discriminative Sentence Modeling for Story Ending Prediction<br>AAAI 20</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;去年开始认真读论文还是从7月开始，主要读了很多Question Answering的论文，其中又以Machine Reading Comprehension为主，大概了解了QA这个领域在做些什么。&lt;br&gt;这里我把去年读过的论文简单列一下。其中我觉得值得一读的论文我会打上😻。&lt;br&gt;阅读过程中尤其感谢LAI Yuxuan师兄的指导！&lt;/p&gt;
&lt;h2 id=&quot;Survey&quot;&gt;&lt;a href=&quot;#Survey&quot; class=&quot;headerlink&quot; title=&quot;Survey&quot;&gt;&lt;/a&gt;Survey&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;NEURAL READING COMPREHENSION AND BEYOND 😻&lt;br&gt;&lt;em&gt;Chen Danqi&lt;/em&gt;&lt;br&gt;陈丹琦的博士毕业论文，一篇对MRC比较全面且易懂的综述。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Neural Machine Reading Comprehension: Methods and Trends&lt;br&gt;&lt;em&gt;Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, Weiming Zhang&lt;/em&gt;&lt;br&gt;国防科大的MRC综述，一些想法很大胆，但一些地方没有讲明白。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches 😻&lt;br&gt;&lt;em&gt;Shane Storks, Qiaozi Gao, Joyce Y. Chai&lt;/em&gt;&lt;br&gt;对于（广义的）Commonsense QA做的综述。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="论文" scheme="http://luciusssss.github.io/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="论文" scheme="http://luciusssss.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="QA" scheme="http://luciusssss.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://luciusssss.github.io/2020/01/11/hello-world/"/>
    <id>http://luciusssss.github.io/2020/01/11/hello-world/</id>
    <published>2020-01-11T05:30:39.878Z</published>
    <updated>2020-01-11T10:38:54.372Z</updated>
    
    <content type="html"><![CDATA[<!-- Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues). --><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you g
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
